{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np                           # Numerical operations\n",
    "import pandas as pd                          # Data loading and manipulation\n",
    "import json                                  # Parsing GeoJSON strings\n",
    "import folium                                # Interactive mapping\n",
    "import joblib                                # Model loading\n",
    "import geopandas as gpd                      # Geospatial data handling\n",
    "from shapely.geometry import Point, box      # Geometry creation\n",
    "import matplotlib.pyplot as plt              # Plotting\n",
    "\n",
    "from sktime.classification.kernel_based import TimeSeriesSVC  # Time series SVM classifier\n",
    "from sktime.dists_kernels import AggrDist       # Aggregated distance kernel wrapper\n",
    "from sklearn.gaussian_process.kernels import RBF  # Radial basis function kernel\n",
    "from sklearn.metrics import f1_score           # Classification metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Data import and date parsing ---\n",
    "# Load time series data from CSV\n",
    "loaded_data = pd.read_csv('path/to/your/data.csv', delimiter=',')\n",
    "# Extract date substring from 'system:index' and convert to datetime\n",
    "loaded_data = loaded_data.assign(date=(loaded_data['system:index'].str[0:9]))\n",
    "loaded_data['date'] = pd.to_datetime(loaded_data['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Generate unique ID from GeoJSON coordinates ---\n",
    "def extract_coordinates(geojson_str):\n",
    "    \"\"\"\n",
    "    Parse a GeoJSON POINT string and return a unique ID formatted as 'lat_lon'.\n",
    "    \"\"\"\n",
    "    geo = json.loads(geojson_str)\n",
    "    lon, lat = geo['coordinates']\n",
    "    return f\"{lat:.6f}_{lon:.6f}\"\n",
    "\n",
    "# Apply coordinate extraction to create ID column\n",
    "loaded_data['ID'] = loaded_data['.geo'].apply(extract_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Filter out cloudy/snowy observations ---\n",
    "filtered_data = loaded_data[loaded_data['NDSI'] < 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Load trained classifier and feature configuration ---\n",
    "model_data = joblib.load('path/to/your/model.pkl')\n",
    "clf = model_data['model']\n",
    "selected_features = model_data['features']\n",
    "multivariate_combinations = model_data['multivariate_combinations']\n",
    "\n",
    "print('Loaded features:', selected_features)\n",
    "print('Available feature combinations:', list(multivariate_combinations.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Choose specific feature combination for prediction ---\n",
    "combination_name = 'NDVI_EVI_FAPAR'\n",
    "if combination_name not in multivariate_combinations:\n",
    "    raise ValueError(f\"Combination {combination_name} not found.\")\n",
    "features_to_use = multivariate_combinations[combination_name]\n",
    "print('Using features:', features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Prepare data for prediction ---\n",
    "# Subset filtered_data to only selected features plus ID and date\n",
    "data_for_pred = filtered_data[features_to_use + ['ID','date']]\n",
    "# Set a MultiIndex (ID, date) for time series input\n",
    "X_new = data_for_pred.set_index(['ID','date']).sort_index()\n",
    "print('Shape of prediction data:', X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Predict cluster/label for each time series ID ---\n",
    "y_pred = clf.predict(X_new)\n",
    "unique_ids = X_new.index.get_level_values('ID').unique()\n",
    "print('Number of unique IDs:', len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of predictions by ID\n",
    "predictions_df = pd.DataFrame({'ID': unique_ids, 'predicted': y_pred})\n",
    "# Merge predictions back into filtered_data\n",
    "filtered_with_pred = filtered_data.merge(predictions_df, on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Extract latitude and longitude for mapping ---\n",
    "def extract_lat_lon(geojson_str):\n",
    "    \"\"\"\n",
    "    Parse a GeoJSON POINT string and return (lat, lon).\n",
    "    \"\"\"\n",
    "    geo = json.loads(geojson_str)\n",
    "    lon, lat = geo['coordinates']\n",
    "    return lat, lon\n",
    "\n",
    "# Apply extraction for mapping\n",
    "filtered_with_pred['lat'], filtered_with_pred['lon'] = zip(*filtered_with_pred['.geo'].apply(extract_lat_lon))\n",
    "print(filtered_with_pred[['lat','lon','predicted']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Aggregate predictions to one per ID ---\n",
    "agg = filtered_with_pred.groupby('ID').agg(\n",
    "    predicted=('predicted', lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]),\n",
    "    lat=('lat','first'),\n",
    "    lon=('lon','first')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create GeoDataFrame of point centres ---\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    agg,\n",
    "    geometry=gpd.points_from_xy(agg.lon, agg.lat),\n",
    "    crs='EPSG:4326'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create square polygons around each point ---\n",
    "pixel_size = 10  # size of square in meters\n",
    "# Reproject to metric CRS for accurate square size\n",
    "gdf_metric = gdf.to_crs(epsg=32633)\n",
    "# Generate square polygons of specified pixel size\n",
    "gdf_metric['geometry'] = gdf_metric.geometry.apply(lambda p: box(p.x - pixel_size/2, p.y - pixel_size/2,\n",
    "                                                               p.x + pixel_size/2, p.y + pixel_size/2))\n",
    "# Save square-pixel shapefile\n",
    "pix_shp = 'model_pixels.shp'\n",
    "gdf_metric.to_file(pix_shp, driver='ESRI Shapefile')\n",
    "print(f\"Square pixel shapefile saved to {pix_shp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
