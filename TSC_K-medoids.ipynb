{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd               # Data manipulation and analysis\n",
    "import numpy as np                # Numerical operations on arrays\n",
    "import matplotlib.pyplot as plt   # Plotting library\n",
    "\n",
    "from matplotlib.lines import Line2D                        # Custom legend creation\n",
    "from tslearn.metrics import cdist_dtw                      # Compute DTW distances\n",
    "from sktime.clustering.k_medoids import TimeSeriesKMedoids # K-medoids clustering for time series\n",
    "from sklearn.preprocessing import MinMaxScaler             # Normalize data\n",
    "from itertools import product                              # Create combinations of parameters\n",
    "from tslearn.clustering import silhouette_score            # Compute silhouette score for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Data import ---\n",
    "# Load CSV containing Sentinel-2 indices\n",
    "data = pd.read_csv('path/to/your/data.csv', delimiter=',')\n",
    "\n",
    "#--- Data exploration ---\n",
    "# Count unique polygons/areas\n",
    "print('Number of areas/polygons:', data['ID'].nunique())\n",
    "\n",
    "#--- Date extraction and conversion ---\n",
    "# Extract date substring and convert to datetime\n",
    "data = data.assign(date=data['system:index'].str[:9])\n",
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Filtering ---\n",
    "# Exclude records with NDSI >= 0.4 (likely clouds or snow)\n",
    "filtered_data = data[data['NDSI'] < 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Prepare 3D ndarray for clustering ---\n",
    "unique_ids = filtered_data['ID'].unique()\n",
    "unique_dates = filtered_data['date'].unique()\n",
    "features = ['NDMI']  # List of features to cluster on\n",
    "\n",
    "n_samples = len(unique_ids)\n",
    "seq_length = len(unique_dates)\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializace ndarray pro v√Ωstup\n",
    "n_samples = len(unique_ids)\n",
    "seq_length = len(unique_dates)\n",
    "n_features = len(features)\n",
    "data_ndarray = np.full((n_samples, seq_length, n_features), np.nan)  \n",
    "seq_length\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize array with NaNs for missing entries\n",
    "data_ndarray = np.full((n_samples, seq_length, n_features), np.nan)\n",
    "\n",
    "# Create mappings from ID/date to array indices\n",
    "id_to_idx = {val: idx for idx, val in enumerate(unique_ids)}\n",
    "date_to_idx = {val: idx for idx, val in enumerate(unique_dates)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate ndarray with observed feature values\n",
    "for _, row in filtered_data.iterrows():\n",
    "    i = id_to_idx[row['ID']]\n",
    "    j = date_to_idx[row['date']]\n",
    "    for f_idx, feature in enumerate(features):\n",
    "        data_ndarray[i, j, f_idx] = row[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Interpolate missing data ---\n",
    "def interpolate_ts(ts):\n",
    "    df_ts = pd.DataFrame(ts, columns=['val'])\n",
    "    # Linear interpolation for both directions\n",
    "    df_ts['val'] = df_ts['val'].interpolate(method='linear', limit_direction='both')\n",
    "    return df_ts['val'].values\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for f in range(n_features):\n",
    "        data_ndarray[i, :, f] = interpolate_ts(data_ndarray[i, :, f])\n",
    "\n",
    "print('NaN count after interpolation:', np.isnan(data_ndarray).sum())\n",
    "print('Data shape:', data_ndarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Min-Max normalization to the range [-1, 1] for each feature separately ---\n",
    "data_reshaped = data_ndarray.reshape(-1, n_features)  # Reshape to 2D for normalization\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_scaled = scaler.fit_transform(data_reshaped)     # Perform normalization\n",
    "\n",
    "# Reshape back to the original 3D shape\n",
    "data_ndarray = data_scaled.reshape(n_samples, seq_length, n_features)\n",
    "\n",
    "print(\"Min-Max normalization completed.\")\n",
    "print(\"New data range:\", data_ndarray.min(), data_ndarray.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter grid definition for sktime TimeSeriesKMedoids ---\n",
    "param_grid = {\n",
    "    \"n_clusters\":     [7],\n",
    "    \"init_algorithm\": [\"kmeans++\", \"random\", \"forgy\"],\n",
    "    \"n_init\":         [10, 100, 250, 500, 750, 1000],\n",
    "    \"max_iter\":       [100, 200, 300, 500],\n",
    "    \"tol\":            [1e-3],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "grid = [dict(zip(keys, combo)) for combo in product(*values)]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_cfg   = None\n",
    "best_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid search with silhouette evaluation ---\n",
    "for params in grid:\n",
    "    model = TimeSeriesKMedoids(\n",
    "        random_state=42,\n",
    "        metric=\"dtw\",\n",
    "        **params\n",
    "    )\n",
    "    labels = model.fit_predict(data_ndarray)\n",
    "    score = silhouette_score(\n",
    "        data_ndarray,\n",
    "        labels,\n",
    "        metric=\"dtw\",\n",
    "    )\n",
    "    print(f\"Testing {params} ‚Üí silhouette score: {score:.4f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_cfg   = params\n",
    "        best_model = model\n",
    "\n",
    "print(f\"\\nüèÜ Best configuration: {best_cfg} with silhouette score = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Results visualization with plot_cluster_algorithm ---\n",
    "# Re-train the model so that centroids are stored in best_model.cluster_centers_\n",
    "best_model = TimeSeriesKMedoids(\n",
    "    random_state=42,\n",
    "    metric=\"dtw\",\n",
    "    **best_cfg\n",
    ").fit(data_ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Predict cluster labels for each time series\n",
    "clusters = best_model.fit_predict(data_ndarray)\n",
    "\n",
    "# --- Visualize clusters ---\n",
    "# Create a colormap and assign distinct colors for each feature\n",
    "cmap = plt.get_cmap('tab10')\n",
    "feature_colors = [cmap(i / n_features) for i in range(n_features)]\n",
    "\n",
    "# Set up one subplot per cluster, arranged vertically\n",
    "fig, axes = plt.subplots(7, 1, figsize=(10, 2 * 7), sharex=True)\n",
    "\n",
    "for cluster_id in range(7):\n",
    "    series_indices = np.where(clusters == cluster_id)[0]\n",
    "    print(f'Cluster {cluster_id + 1}: {len(series_indices)} time series')\n",
    "\n",
    "    ax = axes[cluster_id]\n",
    "    # Plot each time series in the cluster using its feature colors\n",
    "    for idx in series_indices:\n",
    "        for feature_idx in range(n_features):\n",
    "            ax.plot(\n",
    "                unique_dates,\n",
    "                data_ndarray[idx, :, feature_idx],\n",
    "                color=feature_colors[feature_idx],\n",
    "                alpha=0.5,\n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "    ax.set_title(f'Cluster {cluster_id + 1}')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Add legend mapping colors to feature names\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], color=feature_colors[f], lw=2, label=features[f])\n",
    "        for f in range(n_features)\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "# Label the shared x-axis\n",
    "axes[-1].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Export cluster assignments ---\n",
    "cluster_map = pd.DataFrame({'ID': unique_ids, 'Cluster': clusters})\n",
    "export_data = filtered_data[['ID', 'phase']].drop_duplicates().merge(cluster_map, on='ID')\n",
    "export_data.to_csv('clusters.csv', index=False)\n",
    "print('Export completed: clusters.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
