{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd               # Data manipulation and analysis\n",
    "import numpy as np                # Numerical operations on arrays\n",
    "import matplotlib.pyplot as plt   # Plotting library\n",
    "\n",
    "from matplotlib.lines import Line2D                        # Custom legend creation\n",
    "from tslearn.metrics import cdist_dtw                      # Compute DTW distances\n",
    "from sktime.clustering.k_medoids import TimeSeriesKMedoids # K-medoids clustering for time series\n",
    "from kneed import KneeLocator                              # Detect elbow point in curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Data import ---\n",
    "# Load CSV containing Sentinel-2 indices\n",
    "data = pd.read_csv('path/to/your/data.csv', delimiter=',')\n",
    "\n",
    "#--- Data exploration ---\n",
    "# Count unique polygons/areas\n",
    "print('Number of areas/polygons:', data['ID'].nunique())\n",
    "\n",
    "#--- Date extraction and conversion ---\n",
    "# Extract date substring and convert to datetime\n",
    "data = data.assign(date=data['system:index'].str[:9])\n",
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Filtering ---\n",
    "# Exclude records with NDSI >= 0.4 (likely clouds or snow)\n",
    "filtered_data = data[data['NDSI'] < 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Prepare 3D ndarray for clustering ---\n",
    "unique_ids = filtered_data['ID'].unique()\n",
    "unique_dates = filtered_data['date'].unique()\n",
    "features = ['NDMI']  # List of features to cluster on\n",
    "\n",
    "n_samples = len(unique_ids)\n",
    "seq_length = len(unique_dates)\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializace ndarray pro vÃ½stup\n",
    "n_samples = len(unique_ids)\n",
    "seq_length = len(unique_dates)\n",
    "n_features = len(features)\n",
    "data_ndarray = np.full((n_samples, seq_length, n_features), np.nan)  \n",
    "seq_length\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize array with NaNs for missing entries\n",
    "data_ndarray = np.full((n_samples, seq_length, n_features), np.nan)\n",
    "\n",
    "# Create mappings from ID/date to array indices\n",
    "id_to_idx = {val: idx for idx, val in enumerate(unique_ids)}\n",
    "date_to_idx = {val: idx for idx, val in enumerate(unique_dates)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate ndarray with observed feature values\n",
    "for _, row in filtered_data.iterrows():\n",
    "    i = id_to_idx[row['ID']]\n",
    "    j = date_to_idx[row['date']]\n",
    "    for f_idx, feature in enumerate(features):\n",
    "        data_ndarray[i, j, f_idx] = row[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Interpolate missing data ---\n",
    "def interpolate_ts(ts):\n",
    "    df_ts = pd.DataFrame(ts, columns=['val'])\n",
    "    # Linear interpolation for both directions\n",
    "    df_ts['val'] = df_ts['val'].interpolate(method='linear', limit_direction='both')\n",
    "    return df_ts['val'].values\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for f in range(n_features):\n",
    "        data_ndarray[i, :, f] = interpolate_ts(data_ndarray[i, :, f])\n",
    "\n",
    "print('NaN count after interpolation:', np.isnan(data_ndarray).sum())\n",
    "print('Data shape:', data_ndarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Determine optimal number of clusters via elbow method ---\n",
    "Ks = range(2, 25)\n",
    "distortions = []\n",
    "inertias = []\n",
    "for k in Ks:\n",
    "    model = TimeSeriesKMedoids(\n",
    "        n_clusters=k,\n",
    "        metric=\"dtw\",\n",
    "        random_state=42,\n",
    "        n_init=750,\n",
    "        max_iter=500,\n",
    "        tol=1e-6,\n",
    "        verbose=True,\n",
    "        init_algorithm=\"kmeans++\"\n",
    "    )\n",
    "    labels = model.fit_predict(data_ndarray)\n",
    "    # Calculate distortion: mean squared DTW to closest centroid\n",
    "    dists = cdist_dtw(data_ndarray, model.cluster_centers_)\n",
    "    distortions.append(np.mean(np.min(dists, axis=1)**2))\n",
    "    # Record inertia from model\n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distortion and inertia vs. number of clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(Ks, distortions, 'bx-', label='Distortion')\n",
    "plt.plot(Ks, inertias, 'ro-', label='Inertia')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Distortion / Inertia')\n",
    "plt.title('Elbow Method for Time Series Clustering')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find elbow point automatically\n",
    "knee = KneeLocator(Ks, distortions, curve='convex', direction='decreasing').knee\n",
    "print(f'Optimal number of clusters: {knee}')\n",
    "\n",
    "# Plot with elbow point marked\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(Ks, distortions, 'bx-')\n",
    "plt.axvline(x=knee, color='r', linestyle='--', label=f'Elbow: {knee}')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method with Detected Point')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Final clustering with chosen k ---\n",
    "final_model = TimeSeriesKMedoids(\n",
    "    n_clusters=knee,\n",
    "    metric=\"dtw\",\n",
    "    random_state=42,\n",
    "    n_init=750,\n",
    "    max_iter=500,\n",
    "    tol=1e-6,\n",
    "    verbose=True,\n",
    "    init_algorithm=\"kmeans++\"\n",
    ")\n",
    "clusters = final_model.fit_predict(data_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Visualize clusters ---\n",
    "# Prepare a colormap and assign unique colors for each feature\n",
    "cmap = plt.get_cmap('tab10')\n",
    "feature_colors = [cmap(i / n_features) for i in range(n_features)]\n",
    "\n",
    "# Create one subplot per cluster arranged vertically\n",
    "fig, axes = plt.subplots(knee, 1, figsize=(10, 2 * knee), sharex=True)\n",
    "\n",
    "for cid in range(knee):\n",
    "    cluster_indices = np.where(clusters == cid)[0]\n",
    "    print(f'Cluster {cid+1}: {len(cluster_indices)} series')\n",
    "\n",
    "    ax = axes[cid]\n",
    "    # Plot each series feature for the cluster, using its assigned color\n",
    "    for idx in cluster_indices:\n",
    "        for f in range(n_features):\n",
    "            ax.plot(\n",
    "                unique_dates,\n",
    "                data_ndarray[idx, :, f],\n",
    "                color=feature_colors[f],\n",
    "                alpha=0.5,\n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "    ax.set_title(f'Cluster {cid+1}')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Add a legend to map colors to feature names in this subplot\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], color=feature_colors[f], lw=2, label=features[f])\n",
    "        for f in range(n_features)\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "# Shared axis labels for all subplots\n",
    "axes[-1].set_xlabel('Date')\n",
    "fig.text(0.06, 0.5, 'Value', va='center', rotation='vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Export cluster assignments ---\n",
    "cluster_map = pd.DataFrame({'ID': unique_ids, 'Cluster': clusters})\n",
    "export_data = filtered_data[['ID', 'phase']].drop_duplicates().merge(cluster_map, on='ID')\n",
    "export_data.to_csv('clusters.csv', index=False)\n",
    "print('Export completed: clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
